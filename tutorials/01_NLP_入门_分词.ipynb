{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD4yUksNdEowVso6HSRI+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veager/VeNote-Natural-language-processing/blob/main/tutorials/01_NLP_%E5%85%A5%E9%97%A8_%E5%88%86%E8%AF%8D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP 入门\n",
        "\n",
        "使用的库\n",
        "\n",
        "- `NTKL` 库\n",
        "- `jieba` 结巴分词\n",
        "- `scikit-learn` 库下，`sklearn.feature_extraction.text` 模块\n",
        "\n",
        "主要内容\n",
        "\n",
        "- 分词\n",
        "\n",
        "- tf, idf, tf-idf\n",
        "\n",
        "博客地址\n",
        "\n",
        "- NLP 基础：分词，停词，n元语法, 博客园, [website](https://www.cnblogs.com/veager/articles/16288751.html)\n",
        "\n",
        "- 文本特征提取, 博客园, [website](https://www.cnblogs.com/veager/articles/16285476.html)"
      ],
      "metadata": {
        "id": "bcVboio-gn3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "M2VWwPxTV7X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 分词"
      ],
      "metadata": {
        "id": "wVlt9cbchGDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 `NLTK` 库"
      ],
      "metadata": {
        "id": "nQKDfM85hMmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "08DP-Sfgh3nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 分词所必要的数据\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGxOdTgJh6Rl",
        "outputId": "43d22b41-dfab-4157-a8d0-87915b609af5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
        "\n",
        "s = 'Good muffins cost $3.88\\nin New York. Please buy me ... two of them.\\n\\nThanks.'\n",
        "\n",
        "tokens_1 = word_tokenize(s)    # tokens\n",
        "tokens_2 = wordpunct_tokenize(s)\n",
        "print(tokens_1)\n",
        "print(tokens_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-GX08nzhLJI",
        "outputId": "ebf906d8-68fd-44e0-8ea7-e91bc2c01243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', '...', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
            "['Good', 'muffins', 'cost', '$', '3', '.', '88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', '...', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 `jieba` 分词"
      ],
      "metadata": {
        "id": "RjhGjmlPj3l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "doc = \"我来到北京清华大学\"\n",
        "seg_list = jieba.cut(doc, cut_all=True)\n",
        "print(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n",
        "\n",
        "seg_list = jieba.cut(doc, cut_all=False)\n",
        "print(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_hhBwSpGB-8",
        "outputId": "e9b0f9bb-6ed1-412f-9ed8-1a508f99479e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.853 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n",
            "Default Mode: 我/ 来到/ 北京/ 清华大学\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jieba.del_word(\"创新办\")\n",
        "jieba.del_word(\"云计算\")\n",
        "doc= \"李小福是创新办主任也是云计算方面的专家\"\n",
        "seg_list = jieba.cut(doc)\n",
        "print( \"/ \".join(seg_list))\n",
        "\n",
        "jieba.add_word(\"创新办\")\n",
        "jieba.add_word(\"云计算\")\n",
        "seg_list = jieba.cut(doc)\n",
        "print(\"/ \".join(seg_list))"
      ],
      "metadata": {
        "id": "0pe0xXRFjvWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7074a872-bbb3-44b9-ba25-9d708a4ad8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "李小福/ 是/ 创新/ 办/ 主任/ 也/ 是/ 云/ 计算/ 方面/ 的/ 专家\n",
            "李小福/ 是/ 创新办/ 主任/ 也/ 是/ 云计算/ 方面/ 的/ 专家\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 停词"
      ],
      "metadata": {
        "id": "irQkNuXMhfBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 `NLTK` 库"
      ],
      "metadata": {
        "id": "vEQAa1V3i4BS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A0FXl04iv8Z",
        "outputId": "3b21c33e-3134-453f-bd5b-802aaf7486fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmdeSiZkgvJF",
        "outputId": "15235552-d91d-46dd-af20-3a355a657589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'few', 'some', 'up', 'more', 'ain', 'hasn', 'than', 'our', 'into', \"should've\", \"haven't\", 'own', 'll', 'isn', 'nor', 'don', 're', 'an', 'too', 'its', 'and', 'weren', \"wouldn't\", 'y', 'haven', \"won't\", 'about', 'off', 'now', 'then', 'themselves', 'against', 'there', 'under', \"mightn't\", 'here', 'ma', 'shouldn', \"you'll\", 'shan', 'she', 'this', \"you're\", \"shan't\", 'my', 'he', 'being', 'it', 'those', 'is', 'very', 'so', 't', 'itself', 'until', 'just', 'him', 'herself', 'through', 'all', \"couldn't\", 'aren', 'by', 'your', 'can', 'ours', 'how', 'his', 'having', 'hadn', 'm', 'if', 'theirs', 'has', 'each', 'why', 'when', 'again', 'o', \"mustn't\", 'as', 'you', \"hasn't\", 'where', 'not', 'these', 'while', \"shouldn't\", 'because', 'doesn', \"you'd\", 'down', 'that', 'once', 'both', \"isn't\", 'over', 'yourself', 'whom', 'mustn', 'd', \"weren't\", 'were', 'her', 'will', 'a', 'did', 'hers', 'doing', 'after', 'myself', 'needn', 'had', 'i', 'was', 'same', 'wasn', 'only', \"wasn't\", 'am', \"that'll\", 'in', 'didn', 'their', 'before', 'most', \"aren't\", 'any', \"it's\", 'won', \"needn't\", 'other', 'such', 'mightn', 'himself', 'but', 'yours', \"she's\", 'from', \"doesn't\", 'at', 'be', 'have', 'do', 'below', 'ourselves', \"didn't\", 'above', 'does', 'during', 'are', 'with', 'the', 'what', 'for', 'should', 'no', 'on', 'couldn', 'been', 'out', 'me', 'or', 'between', \"you've\", 'who', 'wouldn', 'we', 'them', 'further', 'of', 'to', 's', 'yourselves', \"don't\", \"hadn't\", 've', 'they', 'which'}\n",
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 过滤停词"
      ],
      "metadata": {
        "id": "0nGbUySEkNK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens_1)\n",
        "\n",
        "filtered_tokens = [w for w in tokens_1 if not w in stopwords.words()]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX843885kSYi",
        "outputId": "c3a810f3-2d5b-44ba-f9a6-6205bf24bdb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', '...', 'two', 'of', 'them', '.', 'Thanks', '.']\n",
            "['Good', 'muffins', 'cost', '$', '3.88', 'New', 'York', '.', 'Please', 'buy', '...', 'two', '.', 'Thanks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 `scikit-learn` 库"
      ],
      "metadata": {
        "id": "crG3Il6TjiEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "corpus = ['This is the first document']\n",
        "stop_words = vectorizer.fit(corpus).get_stop_words()\n",
        "\n",
        "print(stop_words)\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P82ivLmkhexz",
        "outputId": "37211e6e-23b3-41b8-b514-78568d835be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frozenset({'few', 'please', 'some', 'up', 'everywhere', 'everyone', 'sometime', 'into', 'own', 'nor', 'whose', 'and', 'herein', 'about', 'somewhere', 'one', 'rather', 'others', 'anyway', 'forty', 'someone', 'being', 'fifty', 'serious', 'ie', 'bottom', 'herself', 'through', 'amoungst', 'by', 'ever', 'can', 'every', 'ltd', 'has', 'when', 'fire', 'mostly', 'nowhere', 'several', 'these', 'whereby', 'name', 'hereupon', 'whom', 'were', 'whoever', 'becoming', 'thereupon', 'amount', 'whatever', 'in', 'hereby', 'formerly', 'their', 'whole', 'whereas', 'fifteen', 'throughout', 'back', 'yours', 'done', 'below', 'whenever', 'nobody', 'are', 'though', 'should', 'go', 'on', 'been', 'system', 'them', 'us', 'to', 'yourselves', 'inc', 'they', 'since', 'cannot', 'more', 'than', 'due', 'whereupon', 're', 'seems', 'latterly', 'de', 'never', 'made', 'then', 'themselves', 'however', 'there', 'under', 'beside', 'thence', 'amongst', 'this', 'behind', 'seeming', 'my', 'it', 'is', 'those', 'around', 'eight', 'so', 'wherever', 'itself', 'empty', 'him', 'all', 'mine', 'your', 'ours', 'well', 'anywhere', 'less', 'as', 'you', 'where', 'still', 'while', 'two', 'thereby', 'both', 'thin', 'over', 'even', 'put', 'had', 'anything', 'alone', 'con', 'was', 'am', 'sometimes', 'other', 'find', 'at', 'together', 'might', 'what', 'no', 'front', 'or', 'who', 'elsewhere', 'fill', 'of', 'becomes', 'which', 'detail', 'move', 'first', 'take', 'our', 'an', 'too', 'its', 'off', 'beyond', 'show', 'twenty', 'latter', 'except', 'somehow', 'here', 'wherein', 'none', 'un', 'he', 'keep', 'very', 'perhaps', 'nothing', 'eg', 'five', 'thereafter', 'whereafter', 'beforehand', 'therein', 'became', 'found', 'how', 'his', 'if', 'per', 'thick', 'each', 'four', 'noone', 'either', 'why', 'last', 'next', 'not', 'seem', 'down', 'could', 'yet', 'eleven', 'ten', 'her', 'will', 'would', 'myself', 'i', 'whence', 'nine', 'before', 'himself', 'often', 'but', 'along', 'always', 'anyhow', 'from', 'have', 'almost', 'meanwhile', 'above', 'during', 'with', 'within', 'third', 'seemed', 'out', 'me', 'upon', 'couldnt', 'many', 'nevertheless', 'else', 'anyone', 'besides', 'thru', 'without', 'hence', 'now', 'much', 'against', 'get', 'top', 'toward', 'she', 'call', 'side', 'namely', 'until', 'least', 'must', 'towards', 'bill', 'thus', 'although', 'also', 'etc', 'mill', 'twelve', 'again', 'sixty', 'cant', 'because', 'sincere', 'may', 'that', 'once', 'among', 'describe', 'yourself', 'interest', 'former', 'a', 'neither', 'enough', 'hers', 'after', 'cry', 'give', 'via', 'hereafter', 'therefore', 'same', 'only', 'everything', 'otherwise', 'whether', 'most', 'any', 'across', 'co', 'such', 'become', 'onto', 'see', 'be', 'three', 'do', 'ourselves', 'afterwards', 'the', 'another', 'for', 'indeed', 'full', 'already', 'part', 'six', 'between', 'hasnt', 'we', 'something', 'further', 'whither', 'hundred', 'moreover'})\n",
            "318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. n-gram"
      ],
      "metadata": {
        "id": "NPOqzP2Kk5Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams, trigrams, bigrams\n",
        "\n",
        "tokens = \"Insurgents killed in ongoing fighting\".split()\n",
        "\n",
        "trigrams_1 = ngrams(tokens, n=3, pad_left=True, left_pad_symbol='</s>')\n",
        "trigrams_2 = trigrams(tokens, pad_left=True, left_pad_symbol='</s>')\n",
        "\n",
        "print(list(tokens))\n",
        "print(list(trigrams_1))     # 得到的 trigrams_1 和 trigrams_2 相同\n",
        "print(list(trigrams_2))"
      ],
      "metadata": {
        "id": "2GV44PMMk-g-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57108cd-a121-419c-eee5-59a31ec7a36b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Insurgents', 'killed', 'in', 'ongoing', 'fighting']\n",
            "[('</s>', '</s>', 'Insurgents'), ('</s>', 'Insurgents', 'killed'), ('Insurgents', 'killed', 'in'), ('killed', 'in', 'ongoing'), ('in', 'ongoing', 'fighting')]\n",
            "[('</s>', '</s>', 'Insurgents'), ('</s>', 'Insurgents', 'killed'), ('Insurgents', 'killed', 'in'), ('killed', 'in', 'ongoing'), ('in', 'ongoing', 'fighting')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 'Good muffins cost $3.88\\nin New York. Please buy me ... two of them.\\n\\nThanks.'\n",
        "trigrams_1 = ngrams(s, n=3, pad_left=True)\n",
        "trigrams_2 = trigrams(s, pad_left=True)\n",
        "\n",
        "print(list(trigrams_1))\n",
        "print(list(trigrams_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mGKqHq3PZzN",
        "outputId": "2b56da3f-5f6a-4d89-c878-67addecd5cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(None, None, 'G'), (None, 'G', 'o'), ('G', 'o', 'o'), ('o', 'o', 'd'), ('o', 'd', ' '), ('d', ' ', 'm'), (' ', 'm', 'u'), ('m', 'u', 'f'), ('u', 'f', 'f'), ('f', 'f', 'i'), ('f', 'i', 'n'), ('i', 'n', 's'), ('n', 's', ' '), ('s', ' ', 'c'), (' ', 'c', 'o'), ('c', 'o', 's'), ('o', 's', 't'), ('s', 't', ' '), ('t', ' ', '$'), (' ', '$', '3'), ('$', '3', '.'), ('3', '.', '8'), ('.', '8', '8'), ('8', '8', '\\n'), ('8', '\\n', 'i'), ('\\n', 'i', 'n'), ('i', 'n', ' '), ('n', ' ', 'N'), (' ', 'N', 'e'), ('N', 'e', 'w'), ('e', 'w', ' '), ('w', ' ', 'Y'), (' ', 'Y', 'o'), ('Y', 'o', 'r'), ('o', 'r', 'k'), ('r', 'k', '.'), ('k', '.', ' '), ('.', ' ', 'P'), (' ', 'P', 'l'), ('P', 'l', 'e'), ('l', 'e', 'a'), ('e', 'a', 's'), ('a', 's', 'e'), ('s', 'e', ' '), ('e', ' ', 'b'), (' ', 'b', 'u'), ('b', 'u', 'y'), ('u', 'y', ' '), ('y', ' ', 'm'), (' ', 'm', 'e'), ('m', 'e', ' '), ('e', ' ', '.'), (' ', '.', '.'), ('.', '.', '.'), ('.', '.', ' '), ('.', ' ', 't'), (' ', 't', 'w'), ('t', 'w', 'o'), ('w', 'o', ' '), ('o', ' ', 'o'), (' ', 'o', 'f'), ('o', 'f', ' '), ('f', ' ', 't'), (' ', 't', 'h'), ('t', 'h', 'e'), ('h', 'e', 'm'), ('e', 'm', '.'), ('m', '.', '\\n'), ('.', '\\n', '\\n'), ('\\n', '\\n', 'T'), ('\\n', 'T', 'h'), ('T', 'h', 'a'), ('h', 'a', 'n'), ('a', 'n', 'k'), ('n', 'k', 's'), ('k', 's', '.')]\n",
            "[(None, None, 'G'), (None, 'G', 'o'), ('G', 'o', 'o'), ('o', 'o', 'd'), ('o', 'd', ' '), ('d', ' ', 'm'), (' ', 'm', 'u'), ('m', 'u', 'f'), ('u', 'f', 'f'), ('f', 'f', 'i'), ('f', 'i', 'n'), ('i', 'n', 's'), ('n', 's', ' '), ('s', ' ', 'c'), (' ', 'c', 'o'), ('c', 'o', 's'), ('o', 's', 't'), ('s', 't', ' '), ('t', ' ', '$'), (' ', '$', '3'), ('$', '3', '.'), ('3', '.', '8'), ('.', '8', '8'), ('8', '8', '\\n'), ('8', '\\n', 'i'), ('\\n', 'i', 'n'), ('i', 'n', ' '), ('n', ' ', 'N'), (' ', 'N', 'e'), ('N', 'e', 'w'), ('e', 'w', ' '), ('w', ' ', 'Y'), (' ', 'Y', 'o'), ('Y', 'o', 'r'), ('o', 'r', 'k'), ('r', 'k', '.'), ('k', '.', ' '), ('.', ' ', 'P'), (' ', 'P', 'l'), ('P', 'l', 'e'), ('l', 'e', 'a'), ('e', 'a', 's'), ('a', 's', 'e'), ('s', 'e', ' '), ('e', ' ', 'b'), (' ', 'b', 'u'), ('b', 'u', 'y'), ('u', 'y', ' '), ('y', ' ', 'm'), (' ', 'm', 'e'), ('m', 'e', ' '), ('e', ' ', '.'), (' ', '.', '.'), ('.', '.', '.'), ('.', '.', ' '), ('.', ' ', 't'), (' ', 't', 'w'), ('t', 'w', 'o'), ('w', 'o', ' '), ('o', ' ', 'o'), (' ', 'o', 'f'), ('o', 'f', ' '), ('f', ' ', 't'), (' ', 't', 'h'), ('t', 'h', 'e'), ('h', 'e', 'm'), ('e', 'm', '.'), ('m', '.', '\\n'), ('.', '\\n', '\\n'), ('\\n', '\\n', 'T'), ('\\n', 'T', 'h'), ('T', 'h', 'a'), ('h', 'a', 'n'), ('a', 'n', 'k'), ('n', 'k', 's'), ('k', 's', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "跳元语法"
      ],
      "metadata": {
        "id": "Fife4xUNP5iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import skipgrams\n",
        "\n",
        "tokens = \"Insurgents killed in ongoing fighting\".split()\n",
        "skipgrams1 = skipgrams(tokens, 3, 2)\n",
        "\n",
        "print(tokens)\n",
        "print(list(skipgrams1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB1UNWXoPzFT",
        "outputId": "b9b0f101-f7d7-4b4d-b40c-ce51978332b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Insurgents', 'killed', 'in', 'ongoing', 'fighting']\n",
            "[('Insurgents', 'killed', 'in'), ('Insurgents', 'killed', 'ongoing'), ('Insurgents', 'killed', 'fighting'), ('Insurgents', 'in', 'ongoing'), ('Insurgents', 'in', 'fighting'), ('Insurgents', 'ongoing', 'fighting'), ('killed', 'in', 'ongoing'), ('killed', 'in', 'fighting'), ('killed', 'ongoing', 'fighting'), ('in', 'ongoing', 'fighting')]\n"
          ]
        }
      ]
    }
  ]
}